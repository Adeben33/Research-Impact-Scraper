{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e3b2344-3194-4c42-ad1c-0c9c5d7a0f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Retrieving data for Yahya Tayalati...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 581\u001b[0m\n\u001b[1;32m    579\u001b[0m profile, _ \u001b[38;5;241m=\u001b[39m get_author_by_user_id(user_id)\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m--> 581\u001b[0m     works \u001b[38;5;241m=\u001b[39m get_scholar_publications(profile)\n\u001b[1;32m    582\u001b[0m     process_author(author_name, profile, works)\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[4], line 334\u001b[0m, in \u001b[0;36mget_scholar_publications\u001b[0;34m(filled_author, max_results)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pub \u001b[38;5;129;01min\u001b[39;00m filled_author\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublications\u001b[39m\u001b[38;5;124m'\u001b[39m, [])[:max_results]:\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         detailed \u001b[38;5;241m=\u001b[39m safe_fill(pub)\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m detailed:\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 314\u001b[0m, in \u001b[0;36msafe_fill\u001b[0;34m(pub, retries, delay)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(retries):\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 314\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m scholarly\u001b[38;5;241m.\u001b[39mfill(pub)\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    316\u001b[0m         logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetry \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempt\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for fill failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/scholarly/_scholarly.py:238\u001b[0m, in \u001b[0;36m_Scholarly.fill\u001b[0;34m(self, object, sections, sortby, publication_limit)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mobject\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontainer_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPublication\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    237\u001b[0m     publication_parser \u001b[38;5;241m=\u001b[39m PublicationParser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__nav)\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28mobject\u001b[39m \u001b[38;5;241m=\u001b[39m publication_parser\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;28mobject\u001b[39m)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/scholarly/publication_parser.py:278\u001b[0m, in \u001b[0;36mPublicationParser.fill\u001b[0;34m(self, publication)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m publication[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m PublicationSource\u001b[38;5;241m.\u001b[39mAUTHOR_PUBLICATION_ENTRY:\n\u001b[1;32m    277\u001b[0m     url \u001b[38;5;241m=\u001b[39m _CITATIONPUB\u001b[38;5;241m.\u001b[39mformat(publication[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor_pub_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 278\u001b[0m     soup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnav\u001b[38;5;241m.\u001b[39m_get_soup(url)\n\u001b[1;32m    279\u001b[0m     publication[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbib\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgsc_oci_title\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m publication[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbib\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\u2026\u001b[39;00m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/scholarly/_navigator.py:239\u001b[0m, in \u001b[0;36mNavigator._get_soup\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_soup\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BeautifulSoup:\n\u001b[1;32m    238\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the BeautifulSoup for a page on scholar.google.com\"\"\"\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m     html \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_page(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://scholar.google.com\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(url))\n\u001b[1;32m    240\u001b[0m     html \u001b[38;5;241m=\u001b[39m html\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xa0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    241\u001b[0m     res \u001b[38;5;241m=\u001b[39m BeautifulSoup(html, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/scholarly/_navigator.py:113\u001b[0m, in \u001b[0;36mNavigator._get_page\u001b[0;34m(self, pagerequest, premium)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     w \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 113\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(w)\n\u001b[1;32m    114\u001b[0m     resp \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mget(pagerequest, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m premium \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:  \u001b[38;5;66;03m# premium methods may contain sensitive information\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from scholarly import scholarly\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "UNPAYWALL_EMAIL = \"adeniyiebenezer33@gmail.com\"\n",
    "DEBUG_MODE = True\n",
    "REFRESH_CACHE = True\n",
    "\n",
    "OUTPUT_DIR = \".\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "LOG_FILE = os.path.join(OUTPUT_DIR, 'research_impact_log.txt')\n",
    "# ---------- LOGGING ----------\n",
    "logging.basicConfig(\n",
    "    filename='LOG_FILE',\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ---------- DOI + PMID HELPERS ----------\n",
    "def clean_doi(doi):\n",
    "    if not doi:\n",
    "        return None\n",
    "    if doi.startswith(\"https://doi.org/\"):\n",
    "        return doi.replace(\"https://doi.org/\", \"\")\n",
    "    if \"doi.org\" not in doi:\n",
    "        return None\n",
    "    return doi\n",
    "\n",
    "\n",
    "def query_doi_from_openalex(title, author=None):\n",
    "    title_clean = re.sub(r'[^\\w\\s]', '', title.lower())[:200]\n",
    "    query = f\"title.search:{title_clean}\"\n",
    "    if author:\n",
    "        query += f\" AND author.display_name.search:{author}\"\n",
    "    url = f\"https://api.openalex.org/works?filter={query}&per-page=1\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        if r.status_code == 200:\n",
    "            results = r.json().get(\"results\", [])\n",
    "            if results:\n",
    "                work = results[0]\n",
    "                return work.get(\"doi\", \"\").replace(\"https://doi.org/\", \"\"), work.get(\"ids\", {}).get(\"pmid\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"OpenAlex DOI lookup failed for '{title}': {e}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def query_doi_from_crossref(title):\n",
    "    url = f\"https://api.crossref.org/works?query.title={requests.utils.quote(title)}&rows=1\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        if r.status_code == 200:\n",
    "            items = r.json()[\"message\"].get(\"items\", [])\n",
    "            if items:\n",
    "                return items[0].get(\"DOI\"), None\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Crossref DOI lookup failed for '{title}': {e}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def get_pmid_from_pubmed(title):\n",
    "    url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"retmode\": \"json\",\n",
    "        \"term\": title\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(url, params=params, timeout=10)\n",
    "        ids = r.json().get(\"esearchresult\", {}).get(\"idlist\", [])\n",
    "        return ids[0] if ids else None\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"PubMed PMID lookup failed for '{title}': {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# ---------- ALTMETRIC & OA ----------\n",
    "def get_altmetric_summary(doi, pmid=None, title=None, altmetric_404_log=None):\n",
    "    url = f\"https://api.altmetric.com/v1/doi/{doi}\"\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        if DEBUG_MODE:\n",
    "            print(f\"🔗 Altmetric URL: {url} — Status: {r.status_code}\")\n",
    "        if r.status_code == 200:\n",
    "            return extract_altmetric_data(r.json())\n",
    "        elif r.status_code == 404:\n",
    "            if title and altmetric_404_log is not None:\n",
    "                altmetric_404_log.append(title)\n",
    "            return get_altmetric_by_pmid(pmid) if pmid else None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Altmetric error for DOI {doi}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_altmetric_by_pmid(pmid):\n",
    "    url = f\"https://api.altmetric.com/v1/pmid/{pmid}\"\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        if DEBUG_MODE:\n",
    "            print(f\"🔗 Altmetric PMID URL: {url} — Status: {r.status_code}\")\n",
    "        if r.status_code == 200:\n",
    "            return extract_altmetric_data(r.json())\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Altmetric error for PMID {pmid}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_altmetric_data(data):\n",
    "    return {\n",
    "        \"altmetric_id\": data.get(\"id\"),\n",
    "        \"score\": data.get(\"score\", 0),\n",
    "        \"counts\": {\n",
    "            'Twitter': data.get('cited_by_tweeters_count', 0),\n",
    "            'Reddit': data.get('cited_by_rdts_count', 0),\n",
    "            'Blogs': data.get('cited_by_feeds_count', 0),\n",
    "            'News': data.get('cited_by_msm_count', 0),\n",
    "            'Facebook': data.get('cited_by_fbwalls_count', 0),\n",
    "            'Wikipedia': data.get('cited_by_wikipedia_count', 0),\n",
    "            'Policy Docs': data.get('cited_by_policy_count', 0)\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- DOAJ CHECK ----------\n",
    "def is_journal_in_doaj(journal_title):\n",
    "    try:\n",
    "        url = f\"https://doaj.org/api/v2/search/journals/{journal_title}\"\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 200 and 'total' in r.json():\n",
    "            return True, \"doaj\"\n",
    "    except:\n",
    "        pass\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# ---------- CORE CHECK ----------\n",
    "def is_in_core_repository(doi):\n",
    "    try:\n",
    "        if not doi:\n",
    "            return None, None\n",
    "        core_indexed_prefixes = [\"10.5281\", \"10.31235\", \"10.1101\", \"10.6084\"]\n",
    "        if any(doi.startswith(prefix) for prefix in core_indexed_prefixes):\n",
    "            return True, \"mocked_core\"\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"CORE check failed for DOI {doi}: {e}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def get_open_access_status(doi):\n",
    "    url = f\"https://api.unpaywall.org/v2/{doi}?email={UNPAYWALL_EMAIL}\"\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 200:\n",
    "            data = r.json()\n",
    "            return data.get(\"is_oa\", False), data.get(\"oa_status\", \"unknown\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unpaywall error for DOI {doi}: {e}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# ---------- HELPERS ----------\n",
    "def tag_keywords(text, keyword_list):\n",
    "    return any(k in text.lower() for k in keyword_list)\n",
    "\n",
    "\n",
    "def has_media_mentions(altmetric):\n",
    "    if not altmetric:\n",
    "        return False\n",
    "    counts = altmetric.get(\"counts\", {})\n",
    "    return any(counts.get(k, 0) > 0 for k in ['News', 'Blogs', 'Policy Docs', 'Facebook', 'Wikipedia'])\n",
    "\n",
    "\n",
    "def is_preprint(venue, doi):\n",
    "    if doi:\n",
    "        return False\n",
    "    preprint_sources = [\"arxiv\", \"biorxiv\", \"medrxiv\", \"ssrn\", \"osf\", \"researchsquare\", \"preprints\"]\n",
    "    return any(src in venue.lower() for src in preprint_sources) if venue else False\n",
    "\n",
    "\n",
    "# ---------- GOOGLE SCHOLAR ----------\n",
    "def get_author_by_user_id(user_id):\n",
    "    try:\n",
    "        author = scholarly.search_author_id(user_id)\n",
    "        filled = scholarly.fill(author)\n",
    "        return filled, filled['name']\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching scholar profile for user ID {user_id}: {e}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# ---------- DOAJ CHECK ----------\n",
    "def is_journal_in_doaj(journal_title):\n",
    "    try:\n",
    "        url = f\"https://doaj.org/api/v2/search/journals/{journal_title}\"\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 200 and 'total' in r.json():\n",
    "            return True, \"doaj\"\n",
    "    except:\n",
    "        pass\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# ---------- CORE CHECK ----------\n",
    "def is_in_core_repository(doi):\n",
    "    try:\n",
    "        if not doi:\n",
    "            return None, None\n",
    "        core_indexed_prefixes = [\"10.5281\", \"10.31235\", \"10.1101\", \"10.6084\"]\n",
    "        if any(doi.startswith(prefix) for prefix in core_indexed_prefixes):\n",
    "            return True, \"mocked_core\"\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"CORE check failed for DOI {doi}: {e}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# ---------- UNPAYWALL CHECK ----------\n",
    "def get_open_access_status_unpaywall(doi):\n",
    "    url = f\"https://api.unpaywall.org/v2/{doi}?email={UNPAYWALL_EMAIL}\"\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 200:\n",
    "            data = r.json()\n",
    "            return data.get(\"is_oa\", False), data.get(\"oa_status\", \"unknown\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Unpaywall error for DOI {doi}: {e}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# ---------- CROSSREF LICENSE CHECK ----------\n",
    "def get_open_access_status_crossref_license(doi):\n",
    "    try:\n",
    "        url = f\"https://api.crossref.org/works/{doi}\"\n",
    "        r = requests.get(url, timeout=10)\n",
    "        if r.status_code == 200:\n",
    "            licenses = r.json()['message'].get(\"license\", [])\n",
    "            if licenses:\n",
    "                return True, licenses[0].get(\"URL\", \"\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Crossref license check failed for {doi}: {e}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# ---------- PREPRINT CHECK ----------\n",
    "def is_preprint_venue(venue, doi):\n",
    "    if doi:\n",
    "        return False, None\n",
    "    preprint_sources = [\"arxiv\", \"biorxiv\", \"medrxiv\", \"ssrn\", \"osf\", \"researchsquare\", \"preprints\"]\n",
    "    if venue:\n",
    "        for src in preprint_sources:\n",
    "            if src in venue.lower():\n",
    "                return True, src\n",
    "    return False, None\n",
    "\n",
    "\n",
    "# ---------- OA BUTTON CHECK ----------\n",
    "def get_open_access_from_oa_button(doi):\n",
    "    try:\n",
    "        url = f\"https://api.openaccessbutton.org/find?id=https://doi.org/{doi}\"\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 200:\n",
    "            data = r.json()\n",
    "            oa_link = data.get(\"data\", {}).get(\"url\")\n",
    "            if oa_link:\n",
    "                return True, oa_link\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"OA Button error for DOI {doi}: {e}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# ---------- COMBINED OA CHECK ----------\n",
    "def get_combined_open_access_status(doi, venue):\n",
    "    oa, source = is_journal_in_doaj(venue)\n",
    "    if oa is not None:\n",
    "        return oa, f\"doaj\"\n",
    "\n",
    "    oa, source = is_in_core_repository(doi)\n",
    "    if oa is not None:\n",
    "        return oa, f\"core:{source}\"\n",
    "\n",
    "    oa, source = get_open_access_status_unpaywall(doi)\n",
    "    if oa is not None:\n",
    "        return oa, f\"unpaywall:{source}\"\n",
    "\n",
    "    oa, source = get_open_access_status_crossref_license(doi)\n",
    "    if oa is not None:\n",
    "        return oa, f\"crossref_license:{source}\"\n",
    "\n",
    "    oa, source = is_preprint_venue(venue, doi)\n",
    "    if oa:\n",
    "        return oa, f\"preprint:{source}\"\n",
    "\n",
    "    oa, source = get_open_access_from_oa_button(doi)\n",
    "    if oa is not None:\n",
    "        return oa, f\"oa_button:{source}\"\n",
    "\n",
    "    return False, \"unknown\"\n",
    "\n",
    "\n",
    "# ---------- SAFE FILL ----------\n",
    "def safe_fill(pub, retries=3, delay=2):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return scholarly.fill(pub)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Retry {attempt + 1} for fill failed: {e}\")\n",
    "            time.sleep(delay)\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_scholar_publications(filled_author, max_results=3000):\n",
    "    # author_dir = filled_author['name'].lower().replace(' ', '_')\n",
    "    author_dir = os.path.join(OUTPUT_DIR, filled_author['name'].lower().replace(' ', '_'))\n",
    "    os.makedirs(author_dir, exist_ok=True)\n",
    "    cache_file = f\"{author_dir}/cached_publications.json\"\n",
    "\n",
    "    if not REFRESH_CACHE and os.path.exists(cache_file):\n",
    "        with open(cache_file, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    publications = []\n",
    "    for pub in filled_author.get('publications', [])[:max_results]:\n",
    "        try:\n",
    "            detailed = safe_fill(pub)\n",
    "            if not detailed:\n",
    "                continue\n",
    "            title = detailed['bib'].get(\"title\", \"Untitled\")\n",
    "            year = detailed['bib'].get(\"pub_year\", \"N/A\")\n",
    "            authors = detailed['bib'].get(\"author\", \"\")\n",
    "            venue = detailed['bib'].get(\"journal\") or detailed['bib'].get(\"venue\") or detailed['bib'].get(\n",
    "                \"pub\") or \"N/A\"\n",
    "            citations = detailed.get(\"num_citations\", 0)\n",
    "            doi = detailed.get(\"pub_url\", \"\")\n",
    "            publications.append({\n",
    "                \"title\": title,\n",
    "                \"year\": year,\n",
    "                \"authors\": authors,\n",
    "                \"venue\": venue,\n",
    "                \"citations\": citations,\n",
    "                \"doi\": doi\n",
    "            })\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to fill publication: {e}\")\n",
    "\n",
    "    try:\n",
    "        with open(cache_file, \"w\") as f:\n",
    "            json.dump(publications, f, indent=2)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to write cache file for {author_dir}: {e}\")\n",
    "\n",
    "    return publications\n",
    "\n",
    "\n",
    "def classify_publication_type(doi, venue, oa_flag):\n",
    "    if is_preprint(venue, doi):\n",
    "        return \"Preprint\"\n",
    "    elif doi and oa_flag:\n",
    "        return \"Open Access\"\n",
    "    elif doi:\n",
    "        return \"Published\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n",
    "def fallback_oa_from_doi_url(doi):\n",
    "    open_domains = [\"plos.org\", \"bmc.org\", \"frontiersin.org\", \"mdpi.com\", \"peerj.com\"]\n",
    "    for domain in open_domains:\n",
    "        if domain in doi:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def refine_open_access_label(is_oa, oa_status):\n",
    "    if not is_oa or oa_status == \"closed\":\n",
    "        return \"Closed\"\n",
    "    elif oa_status == \"gold\":\n",
    "        return \"Gold OA\"\n",
    "    elif oa_status == \"green\":\n",
    "        return \"Green OA\"\n",
    "    elif oa_status == \"hybrid\":\n",
    "        return \"Hybrid OA\"\n",
    "    elif oa_status == \"bronze\":\n",
    "        return \"Bronze OA\"\n",
    "    else:\n",
    "        return \"Unknown OA\"\n",
    "\n",
    "\n",
    "# ---------- PROCESS ----------\n",
    "def process_author(author_name, profile, works):\n",
    "    # safe_name = author_name.lower().replace(' ', '_')\n",
    "    safe_name = os.path.join(OUTPUT_DIR, author_name.lower().replace(' ', '_'))\n",
    "    os.makedirs(safe_name, exist_ok=True)\n",
    "    results = []\n",
    "    altmetric_404_titles = []\n",
    "\n",
    "    # Save author-level metrics\n",
    "    profile_metrics = {\n",
    "        \"Author\": author_name,\n",
    "        \"Citations_All\": profile.get(\"citedby\", 0),\n",
    "        \"Citations_Since2020\": profile.get(\"citedby5y\", 0),\n",
    "        \"h_index_All\": profile.get(\"hindex\", 0),\n",
    "        \"h_index_Since2020\": profile.get(\"hindex5y\", 0),\n",
    "        \"i10_index_All\": profile.get(\"i10index\", 0),\n",
    "        \"i10_index_Since2020\": profile.get(\"i10index5y\", 0)\n",
    "    }\n",
    "    pd.DataFrame([profile_metrics]).to_csv(f\"{safe_name}/metrics.csv\", index=False)\n",
    "    pd.DataFrame([profile_metrics]).to_json(f\"{safe_name}/metrics.csv\", orient=\"records\", indent=2)\n",
    "\n",
    "    for work in works:\n",
    "        title = work.get(\"title\", \"Untitled\")\n",
    "        year = work.get(\"year\", \"N/A\")\n",
    "        authors = work.get(\"authors\", \"\")\n",
    "        venue = work.get(\"venue\", \"N/A\")\n",
    "        citations = work.get(\"citations\", 0)\n",
    "        raw_doi = work.get(\"doi\", \"\")\n",
    "        doi = clean_doi(raw_doi)\n",
    "\n",
    "        pmid = None\n",
    "        if not doi:\n",
    "            doi, pmid = query_doi_from_openalex(title, author_name)\n",
    "            if not doi:\n",
    "                doi, _ = query_doi_from_crossref(title)\n",
    "        if not pmid:\n",
    "            pmid = get_pmid_from_pubmed(title)\n",
    "\n",
    "        if not doi and not pmid:\n",
    "            logging.info(f\"❌ Skipping — No DOI or PMID found for: {title}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"📄 Processing: {title} ({doi if doi else 'No DOI'})\")\n",
    "        altmetric = get_altmetric_summary(doi, pmid, title=title, altmetric_404_log=altmetric_404_titles)\n",
    "        media_flag = has_media_mentions(altmetric)\n",
    "        counts = altmetric.get(\"counts\", {}) if altmetric else {}\n",
    "        oa_flag, oa_status_raw = get_combined_open_access_status(doi, venue)\n",
    "        oa_type = refine_open_access_label(oa_flag, oa_status_raw)\n",
    "\n",
    "        # paper_link = f\"https://doi.org/{doi}\" if doi else raw_doi if raw_doi.startswith(\"http\") else \"N/A\"\n",
    "\n",
    "        is_preprint_flag = is_preprint(venue, doi)\n",
    "\n",
    "        paper_link = (\n",
    "            f\"https://doi.org/{doi}\" if doi else\n",
    "            raw_doi if is_preprint_flag and raw_doi.startswith(\"http\") else\n",
    "            \"N/A\"\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            \"Author\": author_name,\n",
    "            \"Paper Title\": title,\n",
    "            \"Year\": year,\n",
    "            \"Citations\": citations,\n",
    "            \"DOI\": f\"https://doi.org/{doi}\" if doi else \"N/A\",\n",
    "            \"PMID\": pmid,\n",
    "            \"Authors\": authors,\n",
    "            \"Journal\": venue,\n",
    "            \"Altmetric Score\": altmetric.get(\"score\") if altmetric else 0,\n",
    "            \"Twitter Mentions\": counts.get(\"Twitter\", 0),\n",
    "            \"Reddit Mentions\": counts.get(\"Reddit\", 0),\n",
    "            \"News Mentions\": counts.get(\"News\", 0),\n",
    "            \"Blog Mentions\": counts.get(\"Blogs\", 0),\n",
    "            \"Facebook Mentions\": counts.get(\"Facebook\", 0),\n",
    "            \"Wikipedia Mentions\": counts.get(\"Wikipedia\", 0),\n",
    "            \"Policy Mentions\": counts.get(\"Policy Docs\", 0),\n",
    "            \"Media Mentioned\": media_flag,\n",
    "            \"Open Access\": oa_flag,\n",
    "            \"OA Status\": oa_status_raw,\n",
    "            \"Preprint\": is_preprint(venue, doi),\n",
    "            \"Publication Type\": classify_publication_type(doi, venue, oa_flag),\n",
    "            \"Public Health Impact\": tag_keywords(title, public_health_keywords),\n",
    "            \"Capacity Building\": tag_keywords(title, capacity_building_keywords),\n",
    "            \"Paper Link\": paper_link\n",
    "        })\n",
    "        time.sleep(2)\n",
    "\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_csv(f\"{safe_name}/impact_metrics.csv\", index=False)\n",
    "        df.to_json(f\"{safe_name}/impact_metrics.json\", orient=\"records\", indent=2)\n",
    "        print(f\"✅ Finished for {author_name} — {len(results)} papers saved.\")\n",
    "\n",
    "    if altmetric_404_titles:\n",
    "        pd.DataFrame(altmetric_404_titles, columns=[\"Title\"]).to_csv(\n",
    "            f\"{safe_name}/altmetric_404.csv\", index=False\n",
    "        )\n",
    "        print(f\"⚠️ {len(altmetric_404_titles)} papers returned Altmetric 404. Saved to CSV.\")\n",
    "\n",
    "\n",
    "# ---------- KEYWORDS ----------\n",
    "public_health_keywords = [\n",
    "    \"public health\", \"infectious disease\", \"epidemiology\", \"mathematical modeling\",\n",
    "    \"COVID-19\", \"cholera\", \"malaria\", \"pandemic\", \"outbreak\", \"disease mitigation\",\n",
    "    \"early warning systems\", \"community response\", \"health systems\", \"health equity\",\n",
    "    \"vaccination\", \"surveillance\", \"data-driven decision-making\", \"risk communication\",\n",
    "    \"contact tracing\", \"behavior change\", \"public engagement\", \"intervention\", \"awareness\"\n",
    "]\n",
    "\n",
    "capacity_building_keywords = [\n",
    "    \"training\", \"capacity\", \"leadership\", \"sustainability\", \"skills development\", \"education\",\n",
    "    \"data science training\", \"epidemiological training\", \"south-south collaboration\",\n",
    "    \"research network\", \"mentorship\", \"interdisciplinary teams\", \"technology transfer\",\n",
    "    \"local expertise\", \"workforce development\", \"collaborative learning\",\n",
    "    \"public health training\", \"AI and data innovation\", \"institutional strengthening\", \"infrastructure building\"\n",
    "]\n",
    "\n",
    "# ----------AI4PEP AUTHOR DICTIONARY ----------\n",
    "author_dict = {\n",
    "    # \"Jude Dzevela Kong\": \"dPAVmL0AAAAJ\",\n",
    "    # \"Kingsley Badu\": \"de6nT0EAAAAJ\",\n",
    "    # \"Evelyn Kissi\": \"ZsuY1NsAAAAJ\",\n",
    "    # \"Rachel Gorman\": \"6VcJPOEAAAAJ\",\n",
    "    # \"Sylvain Landry Faye\": \"B6hMjn4AAAAJ\",\n",
    "    # \"Bruce Mellado\": \"BTJnR0UAAAAJ\"\n",
    "    # \"Adesina Simon Sodiya\": \"iNnkbzgAAAAJ\",\n",
    "    # \"Riris Andono Ahmad\": \"H3T6XqcAAAAJ\",\n",
    "    # \"Serge Demidenko\": \"0DcFUWkAAAAJ\",\n",
    "    # \"Romulo de Castro\": \"Hi5-8lwAAAAJ\",\n",
    "    # \"Tseren-Onolt Ishdorj\": \"0WHrk08AAAAJ\",\n",
    "    # \"Andre de Carvalho\": \"Jx_5GrgAAAAJ\",\n",
    "    # \"Manuel Colome\": \"aKZ8i6IAAAAJ\",\n",
    "    # \"Cesar Ugarte-Gil\": \"oMSZ_EgAAAAJ\",\n",
    "    # \"Simon Anderson\": \"eVPe_kAAAAAJ\",\n",
    "    # \"Radwan Qasrawi\": \"eVPe_kAAAAAJ\",\n",
    "    # \"Elie Sokhn\": \"xPIHn-MAAAAJ\",\n",
    "    # \"Gelan Ayana\":\"bNK6lMoAAAAJ\"\n",
    "    \"Yahya Tayalati\": \"MuR6AzYAAAAJ\",\n",
    "    # \"Franklin Asiedu-Bekoe\": \"nLQtW2kAAAAJ\",\n",
    "    # \"Michael Owusu\": \"IPTvRYcAAAAJ\",\n",
    "    # \"Christo El Morr\": \"X58b2IAAAAJ\",\n",
    "    # \"Collins Adu\": \"0ujYGxoAAAAJ\",\n",
    "    # \"Rose-Mary Owusuaa Mensah Gyening\": \"pLbPQXkAAAAJ\",\n",
    "    # \"Jerry Kponyoh\": \"feQo2zYAAAAJ\",\n",
    "    # \"Peter Haddawy\": \"lovm5cAAAAAJ\",\n",
    "    # \"Rudith King\": \"eZs2YKwAAAAJ\",\n",
    "    # \"Anuwat Wiratsudakul\": \"wfovEncAAAAJ\",\n",
    "    # \"Gideon Anapey\": \"12TF5uEAAAAJ\",\n",
    "    # \"Sadri Znaidi\": \"qNuluioAAAAJ\",\n",
    "    # \"Dolvara Gunatilaka\": \"b8LUlLkAAAAJ\",\n",
    "    # \"Augustina Sylverken\": \"i4W1CtsAAAAJ\",\n",
    "    # \"Saranath Lawpoolsri\": \"ycuPRikAAAAJ\",\n",
    "    # \"Edmund Yamba\": \"Br4DbcIAAAAJ\",\n",
    "    # \"Patchara Sriwichai\": \"BYW6VxcAAAAJ\",\n",
    "    # \"Ibrahima Dia\": \"SjstYn0AAAAJ\",\n",
    "    # \"Massamba Diouf\": \"9jQ4KJIAAAAJ\",\n",
    "    # \"Halima Diallo\": \"Qd3EZREAAAAJ\",\n",
    "    # \"Vincent Duclos\": \"sqXi04wAAAAJ\",\n",
    "    # \"Pallab Basu\": \"A8upqZoAAAAJ\",\n",
    "    # \"Shamayeta Bhattacharya\": \"JYKiu1YAAAAJ\",\n",
    "    # \"Vongani Chabalala\": \"NjifuRwAAAAJ\",\n",
    "    # \"Mpho Gololo\": \"uYVSpLMAAAAJ\",\n",
    "    # \"Mary Kawonga\": \"hOwZrkAAAAAJ\",\n",
    "    # \"Benjamin Lieberman\": \"Ll1tz1UAAAAJ\",\n",
    "    # \"Edward Nkadimeng\": \"idJhYpUAAAAJ\",\n",
    "    # \"Busisiwe Nkala-Dlamini\": \"oATqSg4AAAAJ\",\n",
    "    # \"Chuene Mosomane\": \"rme82R4AAAAJ\",\n",
    "    # \"Ketema Lemma\": \"TTaX1mcAAAAJ\",\n",
    "    # \"Victor Ngu Ngwa\": \"wjYy0nsAAAAJ\",\n",
    "    # \"Zahra Movahedi Nia\": \"g9EbkyoAAAAJ\",\n",
    "    # \"Hundessa Daba\": \"oXqyAqMAAAAJ\",\n",
    "    # \"Bontu Habtamu\": \"6zRhejEAAAAJ\",\n",
    "    # \"Gashaw Demlew\": \"2Vtu-KsAAAAJ\",\n",
    "    # \"Elbetel Taye\": \"fo-Q3Y0AAAAJ\",\n",
    "    # \"Mikias Alayu\": \"xTS5iYIAAAAJ\"\n",
    "}\n",
    "\n",
    "# ---------- EXECUTION ----------\n",
    "for author_name, user_id in author_dict.items():\n",
    "    print(f\"🔍 Retrieving data for {author_name}...\")\n",
    "    profile, _ = get_author_by_user_id(user_id)\n",
    "    if profile:\n",
    "        works = get_scholar_publications(profile)\n",
    "        process_author(author_name, profile, works)\n",
    "    else:\n",
    "        print(f\"❌ Could not retrieve profile for {author_name}\")\n",
    "\n",
    "print(\"🎉 All authors processed. Check your output folders.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37f06d0-c487-4c00-aee2-8df2ef5977f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Retrieving data for Jude Kong...\n"
     ]
    }
   ],
   "source": [
    "# ---------- IMPORTS AND CONFIG ----------\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from scholarly import scholarly\n",
    "\n",
    "UNPAYWALL_EMAIL = \"adeniyiebenezer33@gmail.com\"\n",
    "DEBUG_MODE = True\n",
    "REFRESH_CACHE = True\n",
    "OUTPUT_DIR = \".\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "LOG_FILE = os.path.join(OUTPUT_DIR, 'research_impact_log.txt')\n",
    "logging.basicConfig(\n",
    "    filename=LOG_FILE,\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "API_SLEEP_TIME = 3  # Add delay between API calls to prevent rate-limiting\n",
    "\n",
    "# ---------- DOI + PMID HELPERS ----------\n",
    "def clean_doi(doi):\n",
    "    if not doi:\n",
    "        return None\n",
    "    if doi.startswith(\"https://doi.org/\"):\n",
    "        return doi.replace(\"https://doi.org/\", \"\")\n",
    "    if \"doi.org\" not in doi:\n",
    "        return None\n",
    "    return doi\n",
    "\n",
    "def query_doi_from_openalex(title, author=None):\n",
    "    title_clean = re.sub(r'[^\\w\\s]', '', title.lower())[:200]\n",
    "    query = f\"title.search:{title_clean}\"\n",
    "    if author:\n",
    "        query += f\" AND author.display_name.search:{author}\"\n",
    "    url = f\"https://api.openalex.org/works?filter={query}&per-page=1\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        time.sleep(API_SLEEP_TIME)\n",
    "        if r.status_code == 200:\n",
    "            results = r.json().get(\"results\", [])\n",
    "            if results:\n",
    "                work = results[0]\n",
    "                return work.get(\"doi\", \"\").replace(\"https://doi.org/\", \"\"), work.get(\"ids\", {}).get(\"pmid\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"OpenAlex DOI lookup failed for '{title}': {e}\")\n",
    "    return None, None\n",
    "\n",
    "def query_doi_from_crossref(title):\n",
    "    url = f\"https://api.crossref.org/works?query.title={requests.utils.quote(title)}&rows=1\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        time.sleep(API_SLEEP_TIME)\n",
    "        if r.status_code == 200:\n",
    "            items = r.json()[\"message\"].get(\"items\", [])\n",
    "            if items:\n",
    "                return items[0].get(\"DOI\"), None\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Crossref DOI lookup failed for '{title}': {e}\")\n",
    "    return None, None\n",
    "\n",
    "def get_pmid_from_pubmed(title):\n",
    "    url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"retmode\": \"json\",\n",
    "        \"term\": title\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(url, params=params, timeout=10)\n",
    "        time.sleep(API_SLEEP_TIME)\n",
    "        ids = r.json().get(\"esearchresult\", {}).get(\"idlist\", [])\n",
    "        return ids[0] if ids else None\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"PubMed PMID lookup failed for '{title}': {e}\")\n",
    "    return None\n",
    "\n",
    "# ---------- IDRC FUNDING CHECK ----------\n",
    "def has_idrc_funding_or_ack(doi, title):\n",
    "    try:\n",
    "        url = f\"https://api.crossref.org/works/{doi}\"\n",
    "        r = requests.get(url, timeout=10)\n",
    "        time.sleep(API_SLEEP_TIME)\n",
    "        if r.status_code == 200:\n",
    "            data = r.json().get(\"message\", {})\n",
    "            funders = data.get(\"funder\", [])\n",
    "            if funders:\n",
    "                for funder in funders:\n",
    "                    if \"idrc\" in funder.get(\"name\", \"\").lower():\n",
    "                        return True\n",
    "                return True\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"CrossRef funding check failed for {doi}: {e}\")\n",
    "\n",
    "    try:\n",
    "        url = \"https://www.ebi.ac.uk/europepmc/webservices/rest/search\"\n",
    "        params = {\n",
    "            \"query\": f'\"{title}\"',\n",
    "            \"format\": \"json\",\n",
    "            \"resultType\": \"core\"\n",
    "        }\n",
    "        r = requests.get(url, params=params, timeout=10)\n",
    "        time.sleep(API_SLEEP_TIME)\n",
    "        results = r.json().get(\"resultList\", {}).get(\"result\", [])\n",
    "        if results:\n",
    "            for res in results:\n",
    "                ack = res.get(\"acknowledgement\", \"\") or res.get(\"fundingText\", \"\")\n",
    "                if \"idrc\" in ack.lower():\n",
    "                    return True\n",
    "                elif ack.strip():\n",
    "                    return True\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"EuropePMC fallback failed for '{title}': {e}\")\n",
    "\n",
    "    return False\n",
    "\n",
    "# ---------- KEYWORDS ----------\n",
    "public_health_keywords = [\"public health\", \"epidemiology\", \"pandemic\"]\n",
    "capacity_building_keywords = [\"training\", \"capacity\", \"mentorship\"]\n",
    "\n",
    "# ---------- PROCESS AUTHOR ----------\n",
    "def process_author(author_name, profile, works):\n",
    "    output_file = os.path.join(OUTPUT_DIR, f\"{author_name.replace(' ', '_')}_idrc_2020_papers.csv\")\n",
    "    results = []\n",
    "    skipped_early = 0\n",
    "\n",
    "    for work in works:\n",
    "        title = work.get(\"title\", \"\")\n",
    "        year = work.get(\"year\", \"\")\n",
    "        try:\n",
    "            if int(year) < 2020:\n",
    "                skipped_early += 1\n",
    "                continue\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        doi = clean_doi(work.get(\"doi\", \"\"))\n",
    "        if not doi:\n",
    "            doi, _ = query_doi_from_openalex(title, author_name)\n",
    "            if not doi:\n",
    "                doi, _ = query_doi_from_crossref(title)\n",
    "\n",
    "        if not has_idrc_funding_or_ack(doi, title):\n",
    "            continue\n",
    "\n",
    "        results.append({\n",
    "            \"Author\": author_name,\n",
    "            \"Title\": title,\n",
    "            \"Year\": year,\n",
    "            \"DOI\": doi\n",
    "        })\n",
    "\n",
    "    if results:\n",
    "        pd.DataFrame(results).to_csv(output_file, index=False)\n",
    "        print(f\"✅ Saved {len(results)} IDRC-funded papers since 2020 for {author_name}.\")\n",
    "    if skipped_early:\n",
    "        print(f\"⏩ Skipped {skipped_early} pre-2020 papers for {author_name}.\")\n",
    "\n",
    "# ---------- AUTHOR DICTIONARY AND RUNNER ----------\n",
    "author_dict = {\n",
    "    \"Jude Kong\": \"dPAVmL0AAAAJ\",\n",
    "    \"Yahya Tayalati\": \"MuR6AzYAAAAJ\",\n",
    "    \"Riris Andono Ahmad\": \"H3T6XqcAAAAJ\",\n",
    "    \"Simon Anderson\": \"eVPe_kAAAAAJ\",\n",
    "    \"Ketema Lemma\": \"TTaX1mcAAAAJ\"\n",
    "}\n",
    "\n",
    "for author_name, user_id in author_dict.items():\n",
    "    print(f\"🔍 Retrieving data for {author_name}...\")\n",
    "    try:\n",
    "        author = scholarly.search_author_id(user_id)\n",
    "        profile = scholarly.fill(author)\n",
    "        works = profile.get('publications', [])\n",
    "        detailed_works = []\n",
    "        for pub in works:\n",
    "            try:\n",
    "                detailed = scholarly.fill(pub)\n",
    "                detailed_works.append({\n",
    "                    \"title\": detailed['bib'].get(\"title\", \"\"),\n",
    "                    \"year\": detailed['bib'].get(\"pub_year\", \"\"),\n",
    "                    \"authors\": detailed['bib'].get(\"author\", \"\"),\n",
    "                    \"venue\": detailed['bib'].get(\"journal\", \"\") or detailed['bib'].get(\"pub\", \"\"),\n",
    "                    \"citations\": detailed.get(\"num_citations\", 0),\n",
    "                    \"doi\": detailed.get(\"pub_url\", \"\")\n",
    "                })\n",
    "                time.sleep(API_SLEEP_TIME)\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Failed to fill a publication for {author_name}: {e}\")\n",
    "        process_author(author_name, profile, detailed_works)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to retrieve profile for {author_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142f5797-41a7-40d4-918f-067f6f0844b4",
   "metadata": {},
   "source": [
    "## new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5762288b-8b8f-4cbf-a59c-049dc917522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from scholarly import scholarly\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "UNPAYWALL_EMAIL = \"adeniyiebenezer33@gmail.com\"\n",
    "DEBUG_MODE = True\n",
    "REFRESH_CACHE = True\n",
    "\n",
    "OUTPUT_DIR = \".\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "LOG_FILE = os.path.join(OUTPUT_DIR, 'research_impact_log.txt')\n",
    "# ---------- LOGGING ----------\n",
    "logging.basicConfig(\n",
    "    filename='LOG_FILE',\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ---------- DOI + PMID HELPERS ----------\n",
    "def clean_doi(doi):\n",
    "    if not doi:\n",
    "        return None\n",
    "    if doi.startswith(\"https://doi.org/\"):\n",
    "        return doi.replace(\"https://doi.org/\", \"\")\n",
    "    if \"doi.org\" not in doi:\n",
    "        return None\n",
    "    return doi\n",
    "\n",
    "\n",
    "def query_doi_from_openalex(title, author=None):\n",
    "    title_clean = re.sub(r'[^\\w\\s]', '', title.lower())[:200]\n",
    "    query = f\"title.search:{title_clean}\"\n",
    "    if author:\n",
    "        query += f\" AND author.display_name.search:{author}\"\n",
    "    url = f\"https://api.openalex.org/works?filter={query}&per-page=1\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        if r.status_code == 200:\n",
    "            results = r.json().get(\"results\", [])\n",
    "            if results:\n",
    "                work = results[0]\n",
    "                return work.get(\"doi\", \"\").replace(\"https://doi.org/\", \"\"), work.get(\"ids\", {}).get(\"pmid\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"OpenAlex DOI lookup failed for '{title}': {e}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def query_doi_from_crossref(title):\n",
    "    url = f\"https://api.crossref.org/works?query.title={requests.utils.quote(title)}&rows=1\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        if r.status_code == 200:\n",
    "            items = r.json()[\"message\"].get(\"items\", [])\n",
    "            if items:\n",
    "                return items[0].get(\"DOI\"), None\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Crossref DOI lookup failed for '{title}': {e}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def get_pmid_from_pubmed(title):\n",
    "    url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"retmode\": \"json\",\n",
    "        \"term\": title\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(url, params=params, timeout=10)\n",
    "        ids = r.json().get(\"esearchresult\", {}).get(\"idlist\", [])\n",
    "        return ids[0] if ids else None\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"PubMed PMID lookup failed for '{title}': {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# ---------- ALTMETRIC & OA ----------\n",
    "def get_altmetric_summary(doi, pmid=None, title=None, altmetric_404_log=None):\n",
    "    url = f\"https://api.altmetric.com/v1/doi/{doi}\"\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        if DEBUG_MODE:\n",
    "            print(f\"🔗 Altmetric URL: {url} — Status: {r.status_code}\")\n",
    "        if r.status_code == 200:\n",
    "            return extract_altmetric_data(r.json())\n",
    "        elif r.status_code == 404:\n",
    "            if title and altmetric_404_log is not None:\n",
    "                altmetric_404_log.append(title)\n",
    "            return get_altmetric_by_pmid(pmid) if pmid else None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Altmetric error for DOI {doi}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_altmetric_by_pmid(pmid):\n",
    "    url = f\"https://api.altmetric.com/v1/pmid/{pmid}\"\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        if DEBUG_MODE:\n",
    "            print(f\"🔗 Altmetric PMID URL: {url} — Status: {r.status_code}\")\n",
    "        if r.status_code == 200:\n",
    "            return extract_altmetric_data(r.json())\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Altmetric error for PMID {pmid}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_altmetric_data(data):\n",
    "    return {\n",
    "        \"altmetric_id\": data.get(\"id\"),\n",
    "        \"score\": data.get(\"score\", 0),\n",
    "        \"counts\": {\n",
    "            'Twitter': data.get('cited_by_tweeters_count', 0),\n",
    "            'Reddit': data.get('cited_by_rdts_count', 0),\n",
    "            'Blogs': data.get('cited_by_feeds_count', 0),\n",
    "            'News': data.get('cited_by_msm_count', 0),\n",
    "            'Facebook': data.get('cited_by_fbwalls_count', 0),\n",
    "            'Wikipedia': data.get('cited_by_wikipedia_count', 0),\n",
    "            'Policy Docs': data.get('cited_by_policy_count', 0)\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- DOAJ CHECK ----------\n",
    "def is_journal_in_doaj(journal_title):\n",
    "    try:\n",
    "        url = f\"https://doaj.org/api/v2/search/journals/{journal_title}\"\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 200 and 'total' in r.json():\n",
    "            return True, \"doaj\"\n",
    "    except:\n",
    "        pass\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# ---------- CORE CHECK ----------\n",
    "def is_in_core_repository(doi):\n",
    "    try:\n",
    "        if not doi:\n",
    "            return None, None\n",
    "        core_indexed_prefixes = [\"10.5281\", \"10.31235\", \"10.1101\", \"10.6084\"]\n",
    "        if any(doi.startswith(prefix) for prefix in core_indexed_prefixes):\n",
    "            return True, \"mocked_core\"\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"CORE check failed for DOI {doi}: {e}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def get_open_access_status(doi):\n",
    "    url = f\"https://api.unpaywall.org/v2/{doi}?email={UNPAYWALL_EMAIL}\"\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 200:\n",
    "            data = r.json()\n",
    "            return data.get(\"is_oa\", False), data.get(\"oa_status\", \"unknown\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unpaywall error for DOI {doi}: {e}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# ---------- HELPERS ----------\n",
    "def tag_keywords(text, keyword_list):\n",
    "    return any(k in text.lower() for k in keyword_list)\n",
    "\n",
    "\n",
    "def has_media_mentions(altmetric):\n",
    "    if not altmetric:\n",
    "        return False\n",
    "    counts = altmetric.get(\"counts\", {})\n",
    "    return any(counts.get(k, 0) > 0 for k in ['News', 'Blogs', 'Policy Docs', 'Facebook', 'Wikipedia'])\n",
    "\n",
    "\n",
    "def is_preprint(venue, doi):\n",
    "    if doi:\n",
    "        return False\n",
    "    preprint_sources = [\"arxiv\", \"biorxiv\", \"medrxiv\", \"ssrn\", \"osf\", \"researchsquare\", \"preprints\"]\n",
    "    return any(src in venue.lower() for src in preprint_sources) if venue else False\n",
    "\n",
    "\n",
    "# ---------- GOOGLE SCHOLAR ----------\n",
    "def get_author_by_user_id(user_id):\n",
    "    try:\n",
    "        author = scholarly.search_author_id(user_id)\n",
    "        filled = scholarly.fill(author)\n",
    "        return filled, filled['name']\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching scholar profile for user ID {user_id}: {e}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# ---------- DOAJ CHECK ----------\n",
    "def is_journal_in_doaj(journal_title):\n",
    "    try:\n",
    "        url = f\"https://doaj.org/api/v2/search/journals/{journal_title}\"\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 200 and 'total' in r.json():\n",
    "            return True, \"doaj\"\n",
    "    except:\n",
    "        pass\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# ---------- CORE CHECK ----------\n",
    "def is_in_core_repository(doi):\n",
    "    try:\n",
    "        if not doi:\n",
    "            return None, None\n",
    "        core_indexed_prefixes = [\"10.5281\", \"10.31235\", \"10.1101\", \"10.6084\"]\n",
    "        if any(doi.startswith(prefix) for prefix in core_indexed_prefixes):\n",
    "            return True, \"mocked_core\"\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"CORE check failed for DOI {doi}: {e}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# ---------- UNPAYWALL CHECK ----------\n",
    "def get_open_access_status_unpaywall(doi):\n",
    "    url = f\"https://api.unpaywall.org/v2/{doi}?email={UNPAYWALL_EMAIL}\"\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 200:\n",
    "            data = r.json()\n",
    "            return data.get(\"is_oa\", False), data.get(\"oa_status\", \"unknown\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Unpaywall error for DOI {doi}: {e}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# ---------- CROSSREF LICENSE CHECK ----------\n",
    "def get_open_access_status_crossref_license(doi):\n",
    "    try:\n",
    "        url = f\"https://api.crossref.org/works/{doi}\"\n",
    "        r = requests.get(url, timeout=10)\n",
    "        if r.status_code == 200:\n",
    "            licenses = r.json()['message'].get(\"license\", [])\n",
    "            if licenses:\n",
    "                return True, licenses[0].get(\"URL\", \"\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Crossref license check failed for {doi}: {e}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# ---------- PREPRINT CHECK ----------\n",
    "def is_preprint_venue(venue, doi):\n",
    "    if doi:\n",
    "        return False, None\n",
    "    preprint_sources = [\"arxiv\", \"biorxiv\", \"medrxiv\", \"ssrn\", \"osf\", \"researchsquare\", \"preprints\"]\n",
    "    if venue:\n",
    "        for src in preprint_sources:\n",
    "            if src in venue.lower():\n",
    "                return True, src\n",
    "    return False, None\n",
    "\n",
    "\n",
    "# ---------- OA BUTTON CHECK ----------\n",
    "def get_open_access_from_oa_button(doi):\n",
    "    try:\n",
    "        url = f\"https://api.openaccessbutton.org/find?id=https://doi.org/{doi}\"\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 200:\n",
    "            data = r.json()\n",
    "            oa_link = data.get(\"data\", {}).get(\"url\")\n",
    "            if oa_link:\n",
    "                return True, oa_link\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"OA Button error for DOI {doi}: {e}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# ---------- COMBINED OA CHECK ----------\n",
    "def get_combined_open_access_status(doi, venue):\n",
    "    oa, source = is_journal_in_doaj(venue)\n",
    "    if oa is not None:\n",
    "        return oa, f\"doaj\"\n",
    "\n",
    "    oa, source = is_in_core_repository(doi)\n",
    "    if oa is not None:\n",
    "        return oa, f\"core:{source}\"\n",
    "\n",
    "    oa, source = get_open_access_status_unpaywall(doi)\n",
    "    if oa is not None:\n",
    "        return oa, f\"unpaywall:{source}\"\n",
    "\n",
    "    oa, source = get_open_access_status_crossref_license(doi)\n",
    "    if oa is not None:\n",
    "        return oa, f\"crossref_license:{source}\"\n",
    "\n",
    "    oa, source = is_preprint_venue(venue, doi)\n",
    "    if oa:\n",
    "        return oa, f\"preprint:{source}\"\n",
    "\n",
    "    oa, source = get_open_access_from_oa_button(doi)\n",
    "    if oa is not None:\n",
    "        return oa, f\"oa_button:{source}\"\n",
    "\n",
    "    return False, \"unknown\"\n",
    "\n",
    "\n",
    "# ---------- SAFE FILL ----------\n",
    "def safe_fill(pub, retries=3, delay=2):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return scholarly.fill(pub)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Retry {attempt + 1} for fill failed: {e}\")\n",
    "            time.sleep(delay)\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_scholar_publications(filled_author, max_results=3000):\n",
    "    author_dir = os.path.join(OUTPUT_DIR, filled_author['name'].lower().replace(' ', '_'))\n",
    "    os.makedirs(author_dir, exist_ok=True)\n",
    "    cache_file = f\"{author_dir}/cached_publications.json\"\n",
    "\n",
    "    if not REFRESH_CACHE and os.path.exists(cache_file):\n",
    "        with open(cache_file, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    publications = []\n",
    "    for pub in filled_author.get('publications', [])[:max_results]:\n",
    "        try:\n",
    "            detailed = safe_fill(pub)\n",
    "            if not detailed:\n",
    "                continue\n",
    "            title = detailed['bib'].get(\"title\", \"Untitled\")\n",
    "            year = detailed['bib'].get(\"pub_year\", \"N/A\")\n",
    "            authors = detailed['bib'].get(\"author\", \"\")\n",
    "            venue = detailed['bib'].get(\"journal\") or detailed['bib'].get(\"venue\") or detailed['bib'].get(\"pub\") or \"N/A\"\n",
    "            citations = detailed.get(\"num_citations\", 0)\n",
    "            doi = detailed.get(\"pub_url\", \"\")\n",
    "\n",
    "            # Filter by publication year >= 2023\n",
    "            try:\n",
    "                if year != \"N/A\" and int(year) < 2023:\n",
    "                    continue\n",
    "            except:\n",
    "                continue  # skip if year cannot be parsed\n",
    "\n",
    "            publications.append({\n",
    "                \"title\": title,\n",
    "                \"year\": year,\n",
    "                \"authors\": authors,\n",
    "                \"venue\": venue,\n",
    "                \"citations\": citations,\n",
    "                \"doi\": doi\n",
    "            })\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to fill publication: {e}\")\n",
    "\n",
    "    try:\n",
    "        with open(cache_file, \"w\") as f:\n",
    "            json.dump(publications, f, indent=2)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to write cache file for {author_dir}: {e}\")\n",
    "\n",
    "    return publications\n",
    "\n",
    "\n",
    "def classify_publication_type(doi, venue, oa_flag):\n",
    "    if is_preprint(venue, doi):\n",
    "        return \"Preprint\"\n",
    "    elif doi and oa_flag:\n",
    "        return \"Open Access\"\n",
    "    elif doi:\n",
    "        return \"Published\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n",
    "def fallback_oa_from_doi_url(doi):\n",
    "    open_domains = [\"plos.org\", \"bmc.org\", \"frontiersin.org\", \"mdpi.com\", \"peerj.com\"]\n",
    "    for domain in open_domains:\n",
    "        if domain in doi:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def refine_open_access_label(is_oa, oa_status):\n",
    "    if not is_oa or oa_status == \"closed\":\n",
    "        return \"Closed\"\n",
    "    elif oa_status == \"gold\":\n",
    "        return \"Gold OA\"\n",
    "    elif oa_status == \"green\":\n",
    "        return \"Green OA\"\n",
    "    elif oa_status == \"hybrid\":\n",
    "        return \"Hybrid OA\"\n",
    "    elif oa_status == \"bronze\":\n",
    "        return \"Bronze OA\"\n",
    "    else:\n",
    "        return \"Unknown OA\"\n",
    "\n",
    "\n",
    "# ---------- PROCESS ----------\n",
    "def process_author(author_name, profile, works):\n",
    "    # safe_name = author_name.lower().replace(' ', '_')\n",
    "    safe_name = os.path.join(OUTPUT_DIR, author_name.lower().replace(' ', '_'))\n",
    "    os.makedirs(safe_name, exist_ok=True)\n",
    "    results = []\n",
    "    altmetric_404_titles = []\n",
    "\n",
    "    # Save author-level metrics\n",
    "    profile_metrics = {\n",
    "        \"Author\": author_name,\n",
    "        \"Citations_All\": profile.get(\"citedby\", 0),\n",
    "        \"Citations_Since2020\": profile.get(\"citedby5y\", 0),\n",
    "        \"h_index_All\": profile.get(\"hindex\", 0),\n",
    "        \"h_index_Since2020\": profile.get(\"hindex5y\", 0),\n",
    "        \"i10_index_All\": profile.get(\"i10index\", 0),\n",
    "        \"i10_index_Since2020\": profile.get(\"i10index5y\", 0)\n",
    "    }\n",
    "    pd.DataFrame([profile_metrics]).to_csv(f\"{safe_name}/metrics.csv\", index=False)\n",
    "    pd.DataFrame([profile_metrics]).to_json(f\"{safe_name}/metrics.csv\", orient=\"records\", indent=2)\n",
    "\n",
    "    for work in works:\n",
    "        title = work.get(\"title\", \"Untitled\")\n",
    "        year = work.get(\"year\", \"N/A\")\n",
    "        authors = work.get(\"authors\", \"\")\n",
    "        venue = work.get(\"venue\", \"N/A\")\n",
    "        citations = work.get(\"citations\", 0)\n",
    "        raw_doi = work.get(\"doi\", \"\")\n",
    "        doi = clean_doi(raw_doi)\n",
    "\n",
    "        pmid = None\n",
    "        if not doi:\n",
    "            doi, pmid = query_doi_from_openalex(title, author_name)\n",
    "            if not doi:\n",
    "                doi, _ = query_doi_from_crossref(title)\n",
    "        if not pmid:\n",
    "            pmid = get_pmid_from_pubmed(title)\n",
    "\n",
    "        if not doi and not pmid:\n",
    "            logging.info(f\"❌ Skipping — No DOI or PMID found for: {title}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"📄 Processing: {title} ({doi if doi else 'No DOI'})\")\n",
    "        altmetric = get_altmetric_summary(doi, pmid, title=title, altmetric_404_log=altmetric_404_titles)\n",
    "        media_flag = has_media_mentions(altmetric)\n",
    "        counts = altmetric.get(\"counts\", {}) if altmetric else {}\n",
    "        oa_flag, oa_status_raw = get_combined_open_access_status(doi, venue)\n",
    "        oa_type = refine_open_access_label(oa_flag, oa_status_raw)\n",
    "\n",
    "        # paper_link = f\"https://doi.org/{doi}\" if doi else raw_doi if raw_doi.startswith(\"http\") else \"N/A\"\n",
    "\n",
    "        is_preprint_flag = is_preprint(venue, doi)\n",
    "\n",
    "        paper_link = (\n",
    "            f\"https://doi.org/{doi}\" if doi else\n",
    "            raw_doi if is_preprint_flag and raw_doi.startswith(\"http\") else\n",
    "            \"N/A\"\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            \"Author\": author_name,\n",
    "            \"Paper Title\": title,\n",
    "            \"Year\": year,\n",
    "            \"Citations\": citations,\n",
    "            \"DOI\": f\"https://doi.org/{doi}\" if doi else \"N/A\",\n",
    "            \"PMID\": pmid,\n",
    "            \"Authors\": authors,\n",
    "            \"Journal\": venue,\n",
    "            \"Altmetric Score\": altmetric.get(\"score\") if altmetric else 0,\n",
    "            \"Twitter Mentions\": counts.get(\"Twitter\", 0),\n",
    "            \"Reddit Mentions\": counts.get(\"Reddit\", 0),\n",
    "            \"News Mentions\": counts.get(\"News\", 0),\n",
    "            \"Blog Mentions\": counts.get(\"Blogs\", 0),\n",
    "            \"Facebook Mentions\": counts.get(\"Facebook\", 0),\n",
    "            \"Wikipedia Mentions\": counts.get(\"Wikipedia\", 0),\n",
    "            \"Policy Mentions\": counts.get(\"Policy Docs\", 0),\n",
    "            \"Media Mentioned\": media_flag,\n",
    "            \"Open Access\": oa_flag,\n",
    "            \"OA Status\": oa_status_raw,\n",
    "            \"Preprint\": is_preprint(venue, doi),\n",
    "            \"Publication Type\": classify_publication_type(doi, venue, oa_flag),\n",
    "            \"Public Health Impact\": tag_keywords(title, public_health_keywords),\n",
    "            \"Capacity Building\": tag_keywords(title, capacity_building_keywords),\n",
    "            \"Paper Link\": paper_link\n",
    "        })\n",
    "        time.sleep(2)\n",
    "\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_csv(f\"{safe_name}/impact_metrics.csv\", index=False)\n",
    "        df.to_json(f\"{safe_name}/impact_metrics.json\", orient=\"records\", indent=2)\n",
    "        print(f\"✅ Finished for {author_name} — {len(results)} papers saved.\")\n",
    "\n",
    "    if altmetric_404_titles:\n",
    "        pd.DataFrame(altmetric_404_titles, columns=[\"Title\"]).to_csv(\n",
    "            f\"{safe_name}/altmetric_404.csv\", index=False\n",
    "        )\n",
    "        print(f\"⚠️ {len(altmetric_404_titles)} papers returned Altmetric 404. Saved to CSV.\")\n",
    "\n",
    "\n",
    "# ---------- KEYWORDS ----------\n",
    "public_health_keywords = [\n",
    "    \"public health\", \"infectious disease\", \"epidemiology\", \"mathematical modeling\",\n",
    "    \"COVID-19\", \"cholera\", \"malaria\", \"pandemic\", \"outbreak\", \"disease mitigation\",\n",
    "    \"early warning systems\", \"community response\", \"health systems\", \"health equity\",\n",
    "    \"vaccination\", \"surveillance\", \"data-driven decision-making\", \"risk communication\",\n",
    "    \"contact tracing\", \"behavior change\", \"public engagement\", \"intervention\", \"awareness\"\n",
    "]\n",
    "\n",
    "capacity_building_keywords = [\n",
    "    \"training\", \"capacity\", \"leadership\", \"sustainability\", \"skills development\", \"education\",\n",
    "    \"data science training\", \"epidemiological training\", \"south-south collaboration\",\n",
    "    \"research network\", \"mentorship\", \"interdisciplinary teams\", \"technology transfer\",\n",
    "    \"local expertise\", \"workforce development\", \"collaborative learning\",\n",
    "    \"public health training\", \"AI and data innovation\", \"institutional strengthening\", \"infrastructure building\"\n",
    "]\n",
    "\n",
    "# ----------AI4PEP AUTHOR DICTIONARY ----------\n",
    "author_dict = {\n",
    "    \"Jude Dzevela Kong\": \"dPAVmL0AAAAJ\",\n",
    "    \"Ketema Lemma\": \"TTaX1mcAAAAJ\",\n",
    "    \"Victor Ngu Ngwa\": \"wjYy0nsAAAAJ\",\n",
    "    \"Zahra Movahedi Nia\": \"g9EbkyoAAAAJ\",\n",
    "    \"Hundessa Daba\": \"oXqyAqMAAAAJ\",\n",
    "    \"Bontu Habtamu\": \"6zRhejEAAAAJ\",\n",
    "    \"Gashaw Demlew\": \"2Vtu-KsAAAAJ\",\n",
    "    \"Elbetel Taye\": \"fo-Q3Y0AAAAJ\",\n",
    "    \"Mikias Alayu\": \"xTS5iYIAAAAJ\",\n",
    "    \"Kingsley Badu\": \"de6nT0EAAAAJ\",\n",
    "    \"Franklin Asiedu-Bekoe\": \"nLQtW2kAAAAJ\",\n",
    "    \"Michael Owusu\": \"IPTvRYcAAAAJ\",\n",
    "    \"Evelyn Kissi\": \"ZsuY1NsAAAAJ\",\n",
    "    \"Christo El Morr\": \"_X58b2IAAAAJ\",\n",
    "    \"Collins Adu\": \"0ujYGxoAAAAJ\",\n",
    "    \"Rose-Mary Owusuaa Mensah Gyening\": \"pLbPQXkAAAAJ\",\n",
    "    \"Jerry Kponyoh\": \"feQo2zYAAAAJ\",\n",
    "    \"Peter Haddawy\": \"lovm5cAAAAAJ\",\n",
    "    \"Rudith King\": \"eZs2YKwAAAAJ\",\n",
    "    \"Anuwat Wiratsudakul\": \"wfovEncAAAAJ\",\n",
    "    \"Rachel Gorman\": \"6VcJPOEAAAAJ\",\n",
    "    \"Gideon Anapey\": \"12TF5uEAAAAJ\",\n",
    "    \"Sylvain Landry Faye\": \"B6hMjn4AAAAJ\",\n",
    "    \"Bruce Mellado\": \"BTJnR0UAAAAJ\",\n",
    "    \"Adesina Simon Sodiya\": \"iNnkbzgAAAAJ\",\n",
    "    \"Riris Andono Ahmad\": \"H3T6XqcAAAAJ\",\n",
    "    \"Serge Demidenko\": \"0DcFUWkAAAAJ\",\n",
    "    \"Romulo de Castro\": \"Hi5-8lwAAAAJ\",\n",
    "    \"Tseren-Onolt Ishdorj\": \"0WHrk08AAAAJ\",\n",
    "    \"Andre de Carvalho\": \"Jx_5GrgAAAAJ\",\n",
    "    \"Manuel Colome\": \"aKZ8i6IAAAAJ\",\n",
    "    \"Cesar Ugarte-Gil\": \"oMSZ_EgAAAAJ\",\n",
    "    \"Simon Anderson\": \"eVPe_kAAAAAJ\",\n",
    "    \"Radwan Qasrawi\": \"eVPe_kAAAAAJ\",\n",
    "    \"Elie Sokhn\": \"xPIHn-MAAAAJ\",\n",
    "    \"Yahya Tayalati\": \"MuR6AzYAAAAJ\",\n",
    "    \"Sadri Znaidi\": \"qNuluioAAAAJ\",\n",
    "    \"Dolvara Gunatilaka\": \"b8LUlLkAAAAJ\",\n",
    "    \"Augustina Sylverken\": \"i4W1CtsAAAAJ\",\n",
    "    \"Saranath Lawpoolsri\": \"ycuPRikAAAAJ\",\n",
    "    \"Edmund Yamba\": \"Br4DbcIAAAAJ\",\n",
    "    \"Patchara Sriwichai\": \"BYW6VxcAAAAJ\",\n",
    "    \"Ibrahima Dia\": \"SjstYn0AAAAJ\",\n",
    "    \"Massamba Diouf\": \"9jQ4KJIAAAAJ\",\n",
    "    \"Halima Diallo\": \"Qd3EZREAAAAJ\",\n",
    "    \"Vincent Duclos\": \"sqXi04wAAAAJ\",\n",
    "    \"Pallab Basu\": \"A8upqZoAAAAJ\",\n",
    "    \"Shamayeta Bhattacharya\": \"JYKiu1YAAAAJ\",\n",
    "    \"Vongani Chabalala\": \"NjifuRwAAAAJ\",\n",
    "    \"Mpho Gololo\": \"uYVSpLMAAAAJ\",\n",
    "    \"Mary Kawonga\": \"hOwZrkAAAAAJ\",\n",
    "    \"Benjamin Lieberman\": \"Ll1tz1UAAAAJ\",\n",
    "    \"Edward Nkadimeng\": \"idJhYpUAAAAJ\",\n",
    "    \"Busisiwe Nkala-Dlamini\": \"oATqSg4AAAAJ\",\n",
    "    \"Chuene Mosomane\": \"rme82R4AAAAJ\"\n",
    "}\n",
    "\n",
    "# ---------- EXECUTION ----------\n",
    "for author_name, user_id in author_dict.items():\n",
    "    print(f\"🔍 Retrieving data for {author_name}...\")\n",
    "    profile, _ = get_author_by_user_id(user_id)\n",
    "    if profile:\n",
    "        works = get_scholar_publications(profile)\n",
    "        process_author(author_name, profile, works)\n",
    "    else:\n",
    "        print(f\"❌ Could not retrieve profile for {author_name}\")\n",
    "\n",
    "print(\"🎉 All authors processed. Check your output folders.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
