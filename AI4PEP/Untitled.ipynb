{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b2344-3194-4c42-ad1c-0c9c5d7a0f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Retrieving data for Yahya Tayalati...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from scholarly import scholarly\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "UNPAYWALL_EMAIL = \"adeniyiebenezer33@gmail.com\"\n",
    "DEBUG_MODE = True\n",
    "REFRESH_CACHE = True\n",
    "\n",
    "OUTPUT_DIR = \".\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "LOG_FILE = os.path.join(OUTPUT_DIR, 'research_impact_log.txt')\n",
    "# ---------- LOGGING ----------\n",
    "logging.basicConfig(\n",
    "    filename='LOG_FILE',\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ---------- DOI + PMID HELPERS ----------\n",
    "def clean_doi(doi):\n",
    "    if not doi:\n",
    "        return None\n",
    "    if doi.startswith(\"https://doi.org/\"):\n",
    "        return doi.replace(\"https://doi.org/\", \"\")\n",
    "    if \"doi.org\" not in doi:\n",
    "        return None\n",
    "    return doi\n",
    "\n",
    "\n",
    "def query_doi_from_openalex(title, author=None):\n",
    "    title_clean = re.sub(r'[^\\w\\s]', '', title.lower())[:200]\n",
    "    query = f\"title.search:{title_clean}\"\n",
    "    if author:\n",
    "        query += f\" AND author.display_name.search:{author}\"\n",
    "    url = f\"https://api.openalex.org/works?filter={query}&per-page=1\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        if r.status_code == 200:\n",
    "            results = r.json().get(\"results\", [])\n",
    "            if results:\n",
    "                work = results[0]\n",
    "                return work.get(\"doi\", \"\").replace(\"https://doi.org/\", \"\"), work.get(\"ids\", {}).get(\"pmid\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"OpenAlex DOI lookup failed for '{title}': {e}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def query_doi_from_crossref(title):\n",
    "    url = f\"https://api.crossref.org/works?query.title={requests.utils.quote(title)}&rows=1\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        if r.status_code == 200:\n",
    "            items = r.json()[\"message\"].get(\"items\", [])\n",
    "            if items:\n",
    "                return items[0].get(\"DOI\"), None\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Crossref DOI lookup failed for '{title}': {e}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def get_pmid_from_pubmed(title):\n",
    "    url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"retmode\": \"json\",\n",
    "        \"term\": title\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(url, params=params, timeout=10)\n",
    "        ids = r.json().get(\"esearchresult\", {}).get(\"idlist\", [])\n",
    "        return ids[0] if ids else None\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"PubMed PMID lookup failed for '{title}': {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# ---------- ALTMETRIC & OA ----------\n",
    "def get_altmetric_summary(doi, pmid=None, title=None, altmetric_404_log=None):\n",
    "    url = f\"https://api.altmetric.com/v1/doi/{doi}\"\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        if DEBUG_MODE:\n",
    "            print(f\"🔗 Altmetric URL: {url} — Status: {r.status_code}\")\n",
    "        if r.status_code == 200:\n",
    "            return extract_altmetric_data(r.json())\n",
    "        elif r.status_code == 404:\n",
    "            if title and altmetric_404_log is not None:\n",
    "                altmetric_404_log.append(title)\n",
    "            return get_altmetric_by_pmid(pmid) if pmid else None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Altmetric error for DOI {doi}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_altmetric_by_pmid(pmid):\n",
    "    url = f\"https://api.altmetric.com/v1/pmid/{pmid}\"\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        if DEBUG_MODE:\n",
    "            print(f\"🔗 Altmetric PMID URL: {url} — Status: {r.status_code}\")\n",
    "        if r.status_code == 200:\n",
    "            return extract_altmetric_data(r.json())\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Altmetric error for PMID {pmid}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_altmetric_data(data):\n",
    "    return {\n",
    "        \"altmetric_id\": data.get(\"id\"),\n",
    "        \"score\": data.get(\"score\", 0),\n",
    "        \"counts\": {\n",
    "            'Twitter': data.get('cited_by_tweeters_count', 0),\n",
    "            'Reddit': data.get('cited_by_rdts_count', 0),\n",
    "            'Blogs': data.get('cited_by_feeds_count', 0),\n",
    "            'News': data.get('cited_by_msm_count', 0),\n",
    "            'Facebook': data.get('cited_by_fbwalls_count', 0),\n",
    "            'Wikipedia': data.get('cited_by_wikipedia_count', 0),\n",
    "            'Policy Docs': data.get('cited_by_policy_count', 0)\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- DOAJ CHECK ----------\n",
    "def is_journal_in_doaj(journal_title):\n",
    "    try:\n",
    "        url = f\"https://doaj.org/api/v2/search/journals/{journal_title}\"\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 200 and 'total' in r.json():\n",
    "            return True, \"doaj\"\n",
    "    except:\n",
    "        pass\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# ---------- CORE CHECK ----------\n",
    "def is_in_core_repository(doi):\n",
    "    try:\n",
    "        if not doi:\n",
    "            return None, None\n",
    "        core_indexed_prefixes = [\"10.5281\", \"10.31235\", \"10.1101\", \"10.6084\"]\n",
    "        if any(doi.startswith(prefix) for prefix in core_indexed_prefixes):\n",
    "            return True, \"mocked_core\"\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"CORE check failed for DOI {doi}: {e}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def get_open_access_status(doi):\n",
    "    url = f\"https://api.unpaywall.org/v2/{doi}?email={UNPAYWALL_EMAIL}\"\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 200:\n",
    "            data = r.json()\n",
    "            return data.get(\"is_oa\", False), data.get(\"oa_status\", \"unknown\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unpaywall error for DOI {doi}: {e}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# ---------- HELPERS ----------\n",
    "def tag_keywords(text, keyword_list):\n",
    "    return any(k in text.lower() for k in keyword_list)\n",
    "\n",
    "\n",
    "def has_media_mentions(altmetric):\n",
    "    if not altmetric:\n",
    "        return False\n",
    "    counts = altmetric.get(\"counts\", {})\n",
    "    return any(counts.get(k, 0) > 0 for k in ['News', 'Blogs', 'Policy Docs', 'Facebook', 'Wikipedia'])\n",
    "\n",
    "\n",
    "def is_preprint(venue, doi):\n",
    "    if doi:\n",
    "        return False\n",
    "    preprint_sources = [\"arxiv\", \"biorxiv\", \"medrxiv\", \"ssrn\", \"osf\", \"researchsquare\", \"preprints\"]\n",
    "    return any(src in venue.lower() for src in preprint_sources) if venue else False\n",
    "\n",
    "\n",
    "# ---------- GOOGLE SCHOLAR ----------\n",
    "def get_author_by_user_id(user_id):\n",
    "    try:\n",
    "        author = scholarly.search_author_id(user_id)\n",
    "        filled = scholarly.fill(author)\n",
    "        return filled, filled['name']\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching scholar profile for user ID {user_id}: {e}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# ---------- DOAJ CHECK ----------\n",
    "def is_journal_in_doaj(journal_title):\n",
    "    try:\n",
    "        url = f\"https://doaj.org/api/v2/search/journals/{journal_title}\"\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 200 and 'total' in r.json():\n",
    "            return True, \"doaj\"\n",
    "    except:\n",
    "        pass\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# ---------- CORE CHECK ----------\n",
    "def is_in_core_repository(doi):\n",
    "    try:\n",
    "        if not doi:\n",
    "            return None, None\n",
    "        core_indexed_prefixes = [\"10.5281\", \"10.31235\", \"10.1101\", \"10.6084\"]\n",
    "        if any(doi.startswith(prefix) for prefix in core_indexed_prefixes):\n",
    "            return True, \"mocked_core\"\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"CORE check failed for DOI {doi}: {e}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# ---------- UNPAYWALL CHECK ----------\n",
    "def get_open_access_status_unpaywall(doi):\n",
    "    url = f\"https://api.unpaywall.org/v2/{doi}?email={UNPAYWALL_EMAIL}\"\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 200:\n",
    "            data = r.json()\n",
    "            return data.get(\"is_oa\", False), data.get(\"oa_status\", \"unknown\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Unpaywall error for DOI {doi}: {e}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# ---------- CROSSREF LICENSE CHECK ----------\n",
    "def get_open_access_status_crossref_license(doi):\n",
    "    try:\n",
    "        url = f\"https://api.crossref.org/works/{doi}\"\n",
    "        r = requests.get(url, timeout=10)\n",
    "        if r.status_code == 200:\n",
    "            licenses = r.json()['message'].get(\"license\", [])\n",
    "            if licenses:\n",
    "                return True, licenses[0].get(\"URL\", \"\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Crossref license check failed for {doi}: {e}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# ---------- PREPRINT CHECK ----------\n",
    "def is_preprint_venue(venue, doi):\n",
    "    if doi:\n",
    "        return False, None\n",
    "    preprint_sources = [\"arxiv\", \"biorxiv\", \"medrxiv\", \"ssrn\", \"osf\", \"researchsquare\", \"preprints\"]\n",
    "    if venue:\n",
    "        for src in preprint_sources:\n",
    "            if src in venue.lower():\n",
    "                return True, src\n",
    "    return False, None\n",
    "\n",
    "\n",
    "# ---------- OA BUTTON CHECK ----------\n",
    "def get_open_access_from_oa_button(doi):\n",
    "    try:\n",
    "        url = f\"https://api.openaccessbutton.org/find?id=https://doi.org/{doi}\"\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 200:\n",
    "            data = r.json()\n",
    "            oa_link = data.get(\"data\", {}).get(\"url\")\n",
    "            if oa_link:\n",
    "                return True, oa_link\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"OA Button error for DOI {doi}: {e}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# ---------- COMBINED OA CHECK ----------\n",
    "def get_combined_open_access_status(doi, venue):\n",
    "    oa, source = is_journal_in_doaj(venue)\n",
    "    if oa is not None:\n",
    "        return oa, f\"doaj\"\n",
    "\n",
    "    oa, source = is_in_core_repository(doi)\n",
    "    if oa is not None:\n",
    "        return oa, f\"core:{source}\"\n",
    "\n",
    "    oa, source = get_open_access_status_unpaywall(doi)\n",
    "    if oa is not None:\n",
    "        return oa, f\"unpaywall:{source}\"\n",
    "\n",
    "    oa, source = get_open_access_status_crossref_license(doi)\n",
    "    if oa is not None:\n",
    "        return oa, f\"crossref_license:{source}\"\n",
    "\n",
    "    oa, source = is_preprint_venue(venue, doi)\n",
    "    if oa:\n",
    "        return oa, f\"preprint:{source}\"\n",
    "\n",
    "    oa, source = get_open_access_from_oa_button(doi)\n",
    "    if oa is not None:\n",
    "        return oa, f\"oa_button:{source}\"\n",
    "\n",
    "    return False, \"unknown\"\n",
    "\n",
    "\n",
    "# ---------- SAFE FILL ----------\n",
    "def safe_fill(pub, retries=3, delay=2):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return scholarly.fill(pub)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Retry {attempt + 1} for fill failed: {e}\")\n",
    "            time.sleep(delay)\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_scholar_publications(filled_author, max_results=3000):\n",
    "    # author_dir = filled_author['name'].lower().replace(' ', '_')\n",
    "    author_dir = os.path.join(OUTPUT_DIR, filled_author['name'].lower().replace(' ', '_'))\n",
    "    os.makedirs(author_dir, exist_ok=True)\n",
    "    cache_file = f\"{author_dir}/cached_publications.json\"\n",
    "\n",
    "    if not REFRESH_CACHE and os.path.exists(cache_file):\n",
    "        with open(cache_file, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    publications = []\n",
    "    for pub in filled_author.get('publications', [])[:max_results]:\n",
    "        try:\n",
    "            detailed = safe_fill(pub)\n",
    "            if not detailed:\n",
    "                continue\n",
    "            title = detailed['bib'].get(\"title\", \"Untitled\")\n",
    "            year = detailed['bib'].get(\"pub_year\", \"N/A\")\n",
    "            authors = detailed['bib'].get(\"author\", \"\")\n",
    "            venue = detailed['bib'].get(\"journal\") or detailed['bib'].get(\"venue\") or detailed['bib'].get(\n",
    "                \"pub\") or \"N/A\"\n",
    "            citations = detailed.get(\"num_citations\", 0)\n",
    "            doi = detailed.get(\"pub_url\", \"\")\n",
    "            publications.append({\n",
    "                \"title\": title,\n",
    "                \"year\": year,\n",
    "                \"authors\": authors,\n",
    "                \"venue\": venue,\n",
    "                \"citations\": citations,\n",
    "                \"doi\": doi\n",
    "            })\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to fill publication: {e}\")\n",
    "\n",
    "    try:\n",
    "        with open(cache_file, \"w\") as f:\n",
    "            json.dump(publications, f, indent=2)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to write cache file for {author_dir}: {e}\")\n",
    "\n",
    "    return publications\n",
    "\n",
    "\n",
    "def classify_publication_type(doi, venue, oa_flag):\n",
    "    if is_preprint(venue, doi):\n",
    "        return \"Preprint\"\n",
    "    elif doi and oa_flag:\n",
    "        return \"Open Access\"\n",
    "    elif doi:\n",
    "        return \"Published\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "\n",
    "def fallback_oa_from_doi_url(doi):\n",
    "    open_domains = [\"plos.org\", \"bmc.org\", \"frontiersin.org\", \"mdpi.com\", \"peerj.com\"]\n",
    "    for domain in open_domains:\n",
    "        if domain in doi:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def refine_open_access_label(is_oa, oa_status):\n",
    "    if not is_oa or oa_status == \"closed\":\n",
    "        return \"Closed\"\n",
    "    elif oa_status == \"gold\":\n",
    "        return \"Gold OA\"\n",
    "    elif oa_status == \"green\":\n",
    "        return \"Green OA\"\n",
    "    elif oa_status == \"hybrid\":\n",
    "        return \"Hybrid OA\"\n",
    "    elif oa_status == \"bronze\":\n",
    "        return \"Bronze OA\"\n",
    "    else:\n",
    "        return \"Unknown OA\"\n",
    "\n",
    "\n",
    "# ---------- PROCESS ----------\n",
    "def process_author(author_name, profile, works):\n",
    "    # safe_name = author_name.lower().replace(' ', '_')\n",
    "    safe_name = os.path.join(OUTPUT_DIR, author_name.lower().replace(' ', '_'))\n",
    "    os.makedirs(safe_name, exist_ok=True)\n",
    "    results = []\n",
    "    altmetric_404_titles = []\n",
    "\n",
    "    # Save author-level metrics\n",
    "    profile_metrics = {\n",
    "        \"Author\": author_name,\n",
    "        \"Citations_All\": profile.get(\"citedby\", 0),\n",
    "        \"Citations_Since2020\": profile.get(\"citedby5y\", 0),\n",
    "        \"h_index_All\": profile.get(\"hindex\", 0),\n",
    "        \"h_index_Since2020\": profile.get(\"hindex5y\", 0),\n",
    "        \"i10_index_All\": profile.get(\"i10index\", 0),\n",
    "        \"i10_index_Since2020\": profile.get(\"i10index5y\", 0)\n",
    "    }\n",
    "    pd.DataFrame([profile_metrics]).to_csv(f\"{safe_name}/metrics.csv\", index=False)\n",
    "    pd.DataFrame([profile_metrics]).to_json(f\"{safe_name}/metrics.csv\", orient=\"records\", indent=2)\n",
    "\n",
    "    for work in works:\n",
    "        title = work.get(\"title\", \"Untitled\")\n",
    "        year = work.get(\"year\", \"N/A\")\n",
    "        authors = work.get(\"authors\", \"\")\n",
    "        venue = work.get(\"venue\", \"N/A\")\n",
    "        citations = work.get(\"citations\", 0)\n",
    "        raw_doi = work.get(\"doi\", \"\")\n",
    "        doi = clean_doi(raw_doi)\n",
    "\n",
    "        pmid = None\n",
    "        if not doi:\n",
    "            doi, pmid = query_doi_from_openalex(title, author_name)\n",
    "            if not doi:\n",
    "                doi, _ = query_doi_from_crossref(title)\n",
    "        if not pmid:\n",
    "            pmid = get_pmid_from_pubmed(title)\n",
    "\n",
    "        if not doi and not pmid:\n",
    "            logging.info(f\"❌ Skipping — No DOI or PMID found for: {title}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"📄 Processing: {title} ({doi if doi else 'No DOI'})\")\n",
    "        altmetric = get_altmetric_summary(doi, pmid, title=title, altmetric_404_log=altmetric_404_titles)\n",
    "        media_flag = has_media_mentions(altmetric)\n",
    "        counts = altmetric.get(\"counts\", {}) if altmetric else {}\n",
    "        oa_flag, oa_status_raw = get_combined_open_access_status(doi, venue)\n",
    "        oa_type = refine_open_access_label(oa_flag, oa_status_raw)\n",
    "\n",
    "        # paper_link = f\"https://doi.org/{doi}\" if doi else raw_doi if raw_doi.startswith(\"http\") else \"N/A\"\n",
    "\n",
    "        is_preprint_flag = is_preprint(venue, doi)\n",
    "\n",
    "        paper_link = (\n",
    "            f\"https://doi.org/{doi}\" if doi else\n",
    "            raw_doi if is_preprint_flag and raw_doi.startswith(\"http\") else\n",
    "            \"N/A\"\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            \"Author\": author_name,\n",
    "            \"Paper Title\": title,\n",
    "            \"Year\": year,\n",
    "            \"Citations\": citations,\n",
    "            \"DOI\": f\"https://doi.org/{doi}\" if doi else \"N/A\",\n",
    "            \"PMID\": pmid,\n",
    "            \"Authors\": authors,\n",
    "            \"Journal\": venue,\n",
    "            \"Altmetric Score\": altmetric.get(\"score\") if altmetric else 0,\n",
    "            \"Twitter Mentions\": counts.get(\"Twitter\", 0),\n",
    "            \"Reddit Mentions\": counts.get(\"Reddit\", 0),\n",
    "            \"News Mentions\": counts.get(\"News\", 0),\n",
    "            \"Blog Mentions\": counts.get(\"Blogs\", 0),\n",
    "            \"Facebook Mentions\": counts.get(\"Facebook\", 0),\n",
    "            \"Wikipedia Mentions\": counts.get(\"Wikipedia\", 0),\n",
    "            \"Policy Mentions\": counts.get(\"Policy Docs\", 0),\n",
    "            \"Media Mentioned\": media_flag,\n",
    "            \"Open Access\": oa_flag,\n",
    "            \"OA Status\": oa_status_raw,\n",
    "            \"Preprint\": is_preprint(venue, doi),\n",
    "            \"Publication Type\": classify_publication_type(doi, venue, oa_flag),\n",
    "            \"Public Health Impact\": tag_keywords(title, public_health_keywords),\n",
    "            \"Capacity Building\": tag_keywords(title, capacity_building_keywords),\n",
    "            \"Paper Link\": paper_link\n",
    "        })\n",
    "        time.sleep(2)\n",
    "\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_csv(f\"{safe_name}/impact_metrics.csv\", index=False)\n",
    "        df.to_json(f\"{safe_name}/impact_metrics.json\", orient=\"records\", indent=2)\n",
    "        print(f\"✅ Finished for {author_name} — {len(results)} papers saved.\")\n",
    "\n",
    "    if altmetric_404_titles:\n",
    "        pd.DataFrame(altmetric_404_titles, columns=[\"Title\"]).to_csv(\n",
    "            f\"{safe_name}/altmetric_404.csv\", index=False\n",
    "        )\n",
    "        print(f\"⚠️ {len(altmetric_404_titles)} papers returned Altmetric 404. Saved to CSV.\")\n",
    "\n",
    "\n",
    "# ---------- KEYWORDS ----------\n",
    "public_health_keywords = [\n",
    "    \"public health\", \"infectious disease\", \"epidemiology\", \"mathematical modeling\",\n",
    "    \"COVID-19\", \"cholera\", \"malaria\", \"pandemic\", \"outbreak\", \"disease mitigation\",\n",
    "    \"early warning systems\", \"community response\", \"health systems\", \"health equity\",\n",
    "    \"vaccination\", \"surveillance\", \"data-driven decision-making\", \"risk communication\",\n",
    "    \"contact tracing\", \"behavior change\", \"public engagement\", \"intervention\", \"awareness\"\n",
    "]\n",
    "\n",
    "capacity_building_keywords = [\n",
    "    \"training\", \"capacity\", \"leadership\", \"sustainability\", \"skills development\", \"education\",\n",
    "    \"data science training\", \"epidemiological training\", \"south-south collaboration\",\n",
    "    \"research network\", \"mentorship\", \"interdisciplinary teams\", \"technology transfer\",\n",
    "    \"local expertise\", \"workforce development\", \"collaborative learning\",\n",
    "    \"public health training\", \"AI and data innovation\", \"institutional strengthening\", \"infrastructure building\"\n",
    "]\n",
    "\n",
    "# ----------AI4PEP AUTHOR DICTIONARY ----------\n",
    "author_dict = {\n",
    "    # \"Jude Dzevela Kong\": \"dPAVmL0AAAAJ\",\n",
    "    # \"Kingsley Badu\": \"de6nT0EAAAAJ\",\n",
    "    # \"Evelyn Kissi\": \"ZsuY1NsAAAAJ\",\n",
    "    # \"Rachel Gorman\": \"6VcJPOEAAAAJ\",\n",
    "    # \"Sylvain Landry Faye\": \"B6hMjn4AAAAJ\",\n",
    "    # \"Bruce Mellado\": \"BTJnR0UAAAAJ\"\n",
    "    # \"Adesina Simon Sodiya\": \"iNnkbzgAAAAJ\",\n",
    "    # \"Riris Andono Ahmad\": \"H3T6XqcAAAAJ\",\n",
    "    # \"Serge Demidenko\": \"0DcFUWkAAAAJ\",\n",
    "    # \"Romulo de Castro\": \"Hi5-8lwAAAAJ\",\n",
    "    # \"Tseren-Onolt Ishdorj\": \"0WHrk08AAAAJ\",\n",
    "    # \"Andre de Carvalho\": \"Jx_5GrgAAAAJ\",\n",
    "    # \"Manuel Colome\": \"aKZ8i6IAAAAJ\",\n",
    "    # \"Cesar Ugarte-Gil\": \"oMSZ_EgAAAAJ\",\n",
    "    # \"Simon Anderson\": \"eVPe_kAAAAAJ\",\n",
    "    # \"Radwan Qasrawi\": \"eVPe_kAAAAAJ\",\n",
    "    # \"Elie Sokhn\": \"xPIHn-MAAAAJ\",\n",
    "    # \"Gelan Ayana\":\"bNK6lMoAAAAJ\"\n",
    "    \"Yahya Tayalati\": \"MuR6AzYAAAAJ\",\n",
    "    \"Franklin Asiedu-Bekoe\": \"nLQtW2kAAAAJ\",\n",
    "    \"Michael Owusu\": \"IPTvRYcAAAAJ\",\n",
    "    \"Christo El Morr\": \"X58b2IAAAAJ\",\n",
    "    \"Collins Adu\": \"0ujYGxoAAAAJ\",\n",
    "    \"Rose-Mary Owusuaa Mensah Gyening\": \"pLbPQXkAAAAJ\",\n",
    "    \"Jerry Kponyoh\": \"feQo2zYAAAAJ\",\n",
    "    \"Peter Haddawy\": \"lovm5cAAAAAJ\",\n",
    "    \"Rudith King\": \"eZs2YKwAAAAJ\",\n",
    "    \"Anuwat Wiratsudakul\": \"wfovEncAAAAJ\",\n",
    "    \"Gideon Anapey\": \"12TF5uEAAAAJ\",\n",
    "    \"Sadri Znaidi\": \"qNuluioAAAAJ\",\n",
    "    \"Dolvara Gunatilaka\": \"b8LUlLkAAAAJ\",\n",
    "    \"Augustina Sylverken\": \"i4W1CtsAAAAJ\",\n",
    "    \"Saranath Lawpoolsri\": \"ycuPRikAAAAJ\",\n",
    "    \"Edmund Yamba\": \"Br4DbcIAAAAJ\",\n",
    "    \"Patchara Sriwichai\": \"BYW6VxcAAAAJ\",\n",
    "    \"Ibrahima Dia\": \"SjstYn0AAAAJ\",\n",
    "    \"Massamba Diouf\": \"9jQ4KJIAAAAJ\",\n",
    "    \"Halima Diallo\": \"Qd3EZREAAAAJ\",\n",
    "    \"Vincent Duclos\": \"sqXi04wAAAAJ\",\n",
    "    \"Pallab Basu\": \"A8upqZoAAAAJ\",\n",
    "    \"Shamayeta Bhattacharya\": \"JYKiu1YAAAAJ\",\n",
    "    \"Vongani Chabalala\": \"NjifuRwAAAAJ\",\n",
    "    \"Mpho Gololo\": \"uYVSpLMAAAAJ\",\n",
    "    \"Mary Kawonga\": \"hOwZrkAAAAAJ\",\n",
    "    \"Benjamin Lieberman\": \"Ll1tz1UAAAAJ\",\n",
    "    \"Edward Nkadimeng\": \"idJhYpUAAAAJ\",\n",
    "    \"Busisiwe Nkala-Dlamini\": \"oATqSg4AAAAJ\",\n",
    "    \"Chuene Mosomane\": \"rme82R4AAAAJ\",\n",
    "    \"Ketema Lemma\": \"TTaX1mcAAAAJ\",\n",
    "    \"Victor Ngu Ngwa\": \"wjYy0nsAAAAJ\",\n",
    "    \"Zahra Movahedi Nia\": \"g9EbkyoAAAAJ\",\n",
    "    \"Hundessa Daba\": \"oXqyAqMAAAAJ\",\n",
    "    \"Bontu Habtamu\": \"6zRhejEAAAAJ\",\n",
    "    \"Gashaw Demlew\": \"2Vtu-KsAAAAJ\",\n",
    "    \"Elbetel Taye\": \"fo-Q3Y0AAAAJ\",\n",
    "    \"Mikias Alayu\": \"xTS5iYIAAAAJ\"\n",
    "}\n",
    "\n",
    "# ---------- EXECUTION ----------\n",
    "for author_name, user_id in author_dict.items():\n",
    "    print(f\"🔍 Retrieving data for {author_name}...\")\n",
    "    profile, _ = get_author_by_user_id(user_id)\n",
    "    if profile:\n",
    "        works = get_scholar_publications(profile)\n",
    "        process_author(author_name, profile, works)\n",
    "    else:\n",
    "        print(f\"❌ Could not retrieve profile for {author_name}\")\n",
    "\n",
    "print(\"🎉 All authors processed. Check your output folders.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37f06d0-c487-4c00-aee2-8df2ef5977f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
